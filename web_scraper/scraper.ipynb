{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser options\n",
    "option=webdriver.ChromeOptions()\n",
    "option.add_argument(\"--incognito\")\n",
    "# option.add_argument('--headless=chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search specifications\n",
    "position = 'Software+Engineer'\n",
    "location = 'Kuala+Lumpur'\n",
    "\n",
    "within_radius = 15 # within 15 miles of location [15, 25]\n",
    "prevent_dupe = 1 # filter set to 1 [0, 1]\n",
    "days_ago = 7 # posted 7 days ago [1, 3, 7, 14]\n",
    "start_page = 0 # start at page 0\n",
    "\n",
    "pagination_url = 'https://malaysia.indeed.com/jobs?q={}&l={}&radius={}&filter={}&sort=date&fromage={}&start={}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter your query here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "job_list = [] # store job list\n",
    "job_desc_href = [] # store job description link\n",
    "job_descs = [] # store job description\n",
    "job_salary = [] # store salary list\n",
    "merged_array = [] # store merged array\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),options=option)\n",
    "\n",
    "driver.get(pagination_url.format(position, location, within_radius, prevent_dupe, days_ago, start_page))\n",
    "\n",
    "# wait for a random seconds before continuing\n",
    "sleep(randint(2, 6))\n",
    "\n",
    "# find the number of jobs\n",
    "p = driver.find_element(By.CLASS_NAME, 'jobsearch-JobCountAndSortPane-jobCount').text\n",
    "p = p.strip(\" jobs\")\n",
    "p = int(p.replace(',',''))\n",
    "max_iter_pgs = p // 15 # each page contains 15 listing, so we find the page number by dividing 15\n",
    "\n",
    "# set loop = 0 if only 1 page of listings\n",
    "if (max_iter_pgs == 0 and p > 0):\n",
    "    maxRange = 1\n",
    "else:\n",
    "    maxRange = max_iter_pgs\n",
    "\n",
    "# scraper\n",
    "for start_page in range(0,maxRange):\n",
    "    retries = 0\n",
    "    while retries < 5:\n",
    "        try:\n",
    "            print(\"Current Page: \", start_page)\n",
    "            driver.get(pagination_url.format(position, location, within_radius, prevent_dupe, days_ago, start_page * 10)) # 0 = page 1, 10 = page 2, etc\n",
    "            sleep(randint(2, 4))\n",
    "\n",
    "            job_page = driver.find_element(By.ID, \"mosaic-jobResults\")\n",
    "            jobs = job_page.find_elements(By.CLASS_NAME, \"job_seen_beacon\")\n",
    "\n",
    "            for jj in jobs:\n",
    "                job_title = jj.find_element(By.CLASS_NAME, \"jobTitle\")\n",
    "\n",
    "                job_list.append([\n",
    "                    job_title.text, # job title\n",
    "                    job_title.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\"), # job link\n",
    "                    job_title.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"id\"), # job id\n",
    "                    jj.find_element(By.CLASS_NAME, \"companyName\").text, # company name\n",
    "                    jj.find_element(By.CLASS_NAME, \"companyLocation\").text, # company location\n",
    "                    jj.find_element(By.CLASS_NAME, \"date\").text # date posted\n",
    "                ])\n",
    "\n",
    "                try:\n",
    "                    job_salary.append(jj.find_element(By.CLASS_NAME, \"salary-snippet-container\").text)\n",
    "                except NoSuchElementException:\n",
    "                    job_salary.append(None)\n",
    "\n",
    "                try:\n",
    "                    pop_up_button = jj.find_element(By.XPATH, '//*[@id=\"mosaic-desktopserpjapopup\"]/div[1]/button')\n",
    "                    pop_up_button.click()\n",
    "                    print(\"pop-up closed\")\n",
    "                    print(\"Result: \" + job_title.text + \" completed!\")\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Result: \" + job_title.text + \" completed!\")\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"No Internet\")\n",
    "            time.sleep(60)\n",
    "            print(\"retrying...\")\n",
    "\n",
    "# extract job description links\n",
    "for i in range(len(job_list)):\n",
    "    job_desc_href.append(job_list[i][1])\n",
    "\n",
    "# extract job description from each link\n",
    "for url in job_desc_href:\n",
    "    retries = 0\n",
    "    while retries < 5:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            sleep(randint(3, 5))\n",
    "            try:\n",
    "                job_descs.append(driver.find_element(By.ID,\"jobDescriptionText\").text)\n",
    "            except:\n",
    "                job_descs.append(None)\n",
    "            print(\"Page: \" + url + \" completed!\")\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error retriving job desc for: \" + url)\n",
    "            time.sleep(60)\n",
    "            retries += 1\n",
    "            print(\"retrying...\")\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "end = time.time()\n",
    "\n",
    "# merge job listing and with corresponding salary\n",
    "for job, salary, desc in zip(job_list, job_salary, job_descs):\n",
    "    merged_array.append(job + [salary] + [desc])\n",
    "\n",
    "# print scraping details\n",
    "print(end - start, 'seconds to complete action!')\n",
    "print('---------------------')\n",
    "print('Max Iterable Pages for this search:', max_iter_pgs)\n",
    "print('Job Count:', p)\n",
    "print('Extracted: ', len(job_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas DF & Save into Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(merged_array, columns = [\n",
    "    'Job Title', 'Job Link', 'Job ID',\n",
    "    'Company Name', 'Location', 'Job Posting',\n",
    "    'Salary', 'Job Description'\n",
    "])\n",
    "\n",
    "# change this path\n",
    "df.to_csv('..//web_scraper//excel//results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
